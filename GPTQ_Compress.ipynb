{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff5ebb0f",
   "metadata": {},
   "source": [
    "# Notebook – GPTQ_Compress.ipynb\n",
    "Convert a Hugging Face model to 4‑bit GPTQ for vLLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b535f84",
   "metadata": {},
   "source": [
    "## Install dependencies\n",
    "```bash\n",
    "pip install auto-gptq transformers accelerate\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27dd4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random, json\n",
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer\n",
    "from auto_gptq import AutoGPTQForCausalLM, GPTQConfig\n",
    "\n",
    "BASE_MODEL = os.getenv(\"BASE_MODEL\", \"mistral-7b-instruct-v0.3\")\n",
    "OUT_DIR = Path(BASE_MODEL + \"-gptq\")\n",
    "OUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "\n",
    "# Build tiny calibration set (replace with domain snippets for best results)\n",
    "calib_sentences = [\"Hello world!\"] * 128\n",
    "\n",
    "quant_cfg = GPTQConfig(\n",
    "    bits=4,\n",
    "    group_size=128,\n",
    "    dataset=calib_sentences,\n",
    "    desc_act=False\n",
    ")\n",
    "\n",
    "model = AutoGPTQForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    quant_config=quant_cfg,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "model.save_pretrained(OUT_DIR, safe_serialization=True)\n",
    "tokenizer.save_pretrained(OUT_DIR)\n",
    "print(f\"Saved GPTQ model to {OUT_DIR}\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
