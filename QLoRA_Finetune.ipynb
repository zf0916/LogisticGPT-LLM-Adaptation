{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "480a296e",
   "metadata": {},
   "source": [
    "# Notebook – QLoRA_Finetune.ipynb\n",
    "Parameter‑efficient fine‑tune with 4‑bit base weights + LoRA adapters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823d6b44",
   "metadata": {},
   "source": [
    "## Install dependencies\n",
    "```bash\n",
    "pip install transformers peft bitsandbytes trl datasets accelerate\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7dbbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pandas as pd, torch\n",
    "from datasets import Dataset\n",
    "from pathlib import Path\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from trl import SFTTrainer\n",
    "\n",
    "BASE_MODEL = os.getenv(\"BASE_MODEL\", \"mistral-7b-instruct-v0.3\")\n",
    "DATA_FILE = Path(\"eval_qa50.csv\")\n",
    "OUTPUT_DIR = Path(\"lora-\" + BASE_MODEL.split('/')[-1])\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "bnb_cfg = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    quantization_config=bnb_cfg,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Build dataset\n",
    "df = pd.read_csv(DATA_FILE)\n",
    "def make_prompt(row):\n",
    "    return f\"CONTEXT:\\n{row.context}\\nQUESTION:\\n{row.question}\\nANSWER:\"\n",
    "dataset = Dataset.from_dict({\n",
    "    \"prompt\": [make_prompt(r) for _, r in df.iterrows()],\n",
    "    \"response\": df[\"answer\"].tolist()\n",
    "})\n",
    "\n",
    "lora_cfg = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"q_proj\",\"v_proj\",\"k_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "model = get_peft_model(model, lora_cfg)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=str(OUTPUT_DIR),\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=3,\n",
    "    fp16=True,\n",
    "    learning_rate=2e-4,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\"\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    dataset_text_field=\"prompt\",\n",
    "    max_seq_length=1024,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    packing=False\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "model.save_pretrained(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "print(f\"Saved LoRA adapters to {OUTPUT_DIR}\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
