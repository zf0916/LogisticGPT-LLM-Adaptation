{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffd48597",
   "metadata": {},
   "source": [
    "# Notebook ⑥ – Q‑GaLore Full‑Parameter Fine‑tuning (Llama‑3 8B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d029ac42",
   "metadata": {},
   "source": [
    "Quantised‐Gradient GaLore lets you *fully* fine‑tune an 8 B model under a single RTX 4060 Ti (16 GB) by:<br>\n",
    "* loading **4‑bit weights** (bitsandbytes NF4)  \n",
    "* keeping **4‑bit gradients** via GaLore’s low‑rank projection  \n",
    "* using an 8‑bit AdamW optimizer\n",
    "\n",
    "---\n",
    "\n",
    "### Packages\n",
    "\n",
    "```bash\n",
    "pip install bitsandbytes==0.43.1             transformers>=4.41.0             accelerate>=0.29.3             datasets trl             galore-pytorch  # https://github.com/amirgholami/GaLore\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c31060c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, torch, pandas as pd\n",
    "from datasets import Dataset\n",
    "from pathlib import Path\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "from galore_torch import GaLoreAdamW8bit  # comes from galore-pytorch\n",
    "from trl import SFTTrainer, DataCollatorForLanguageModeling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1990f4de",
   "metadata": {},
   "source": [
    "### Load 50‑example instruction dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a54d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FILE = Path(\"eval_qa50.csv\")   # created in Notebook ②\n",
    "df = pd.read_csv(DATA_FILE)\n",
    "\n",
    "def to_prompt(row):\n",
    "    return f\"CONTEXT:\\n{row.context}\\nQUESTION:\\n{row.question}\\nANSWER:\"\n",
    "dataset = Dataset.from_dict({\n",
    "    \"prompt\": [to_prompt(r) for _, r in df.iterrows()],\n",
    "    \"response\": df[\"answer\"].tolist()\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81105559",
   "metadata": {},
   "source": [
    "### Load 8 B model in 4‑bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d19684",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_MODEL = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "bnb_cfg = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    quantization_config=bnb_cfg,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "print(f\"Model loaded with {sum(p.numel() for p in model.parameters())/1e6:.0f} M params\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc9d6c3",
   "metadata": {},
   "source": [
    "### Configure Q‑GaLore Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3e4d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "# GaLore hyper‑params\n",
    "galore_cfg = dict(\n",
    "    rank=64,                # low‑rank projection\n",
    "    update_proj_gap=200,    # how often to re‑project\n",
    "    scale=1.0,\n",
    "    proj_type=\"std\",\n",
    "    beta1=0.9,\n",
    "    beta2=0.95,\n",
    "    weight_decay=0.0\n",
    ")\n",
    "\n",
    "def make_param_groups(model):\n",
    "    decay, no_decay = [], []\n",
    "    for n, p in model.named_parameters():\n",
    "        if not p.requires_grad: continue\n",
    "        if p.ndim < 2 or \"norm\" in n or \"bias\" in n:\n",
    "            no_decay.append(p)\n",
    "        else:\n",
    "            decay.append(p)\n",
    "    return [\n",
    "        {\"params\": decay, \"weight_decay\": 0.01},\n",
    "        {\"params\": no_decay, \"weight_decay\": 0.0},\n",
    "    ]\n",
    "\n",
    "optimizer = GaLoreAdamW8bit(\n",
    "    make_param_groups(model),\n",
    "    lr=2e-4,\n",
    "    **galore_cfg\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0aa64ea",
   "metadata": {},
   "source": [
    "### SFTTrainer Loop with GaLore optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9445a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"qgalore-llama3-8b\",\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=3,\n",
    "    bf16=True,\n",
    "    optim=\"adamw_torch\",   # dummy—will be overridden\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\"\n",
    ")\n",
    "\n",
    "collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=collator,\n",
    "    max_seq_length=1024,\n",
    "    packing=False,\n",
    ")\n",
    "\n",
    "# Override optimizer\n",
    "trainer.create_optimizer = lambda *_: optimizer\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model()\n",
    "tokenizer.save_pretrained(\"qgalore-llama3-8b\")\n",
    "print(\"✅ Saved Q‑GaLore fine‑tuned model to qgalore-llama3-8b/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752111fe",
   "metadata": {},
   "source": [
    "## Expected VRAM\n",
    "\n",
    "| Stage | 8 B params | Peak VRAM |\n",
    "|-------|------------|-----------|\n",
    "| Loading 4‑bit weights | ~5 GB | |\n",
    "| Training step (GaLore rank 64, 8‑bit AdamW) | 12‑14 GB | should fit RTX 4060 Ti 16 GB |\n",
    "\n",
    "Adjust `rank` and `update_proj_gap` if you need more headroom.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
