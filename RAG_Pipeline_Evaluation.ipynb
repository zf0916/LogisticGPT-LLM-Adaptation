{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1f64738",
   "metadata": {},
   "source": [
    "# Notebook ③ – RAG Pipeline & Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c67a6b9",
   "metadata": {},
   "source": [
    "Switch tiers by changing `MODEL_NAME` in the first code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cef752f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, torch, time, pandas as pd, wandb\n",
    "from llama_index import (\n",
    "    StorageContext, load_index_from_storage, ServiceContext, set_global_service_context\n",
    ")\n",
    "from llama_index.llms import Vllm\n",
    "from llama_index.evaluation import RagasEvaluator\n",
    "from llama_index.evaluation import AnswerRelevancyEvaluator, FaithfulnessEvaluator, ContextRecallEvaluator\n",
    "\n",
    "MODEL_NAME = os.getenv(\"MODEL_NAME\", \"mistral-7b-instruct-v0.3-gptq\")\n",
    "\n",
    "index = load_index_from_storage(\"lade_chroma\")\n",
    "retriever = index.as_retriever(search_kwargs={\"k\":4})\n",
    "llm = Vllm(\n",
    "    model=MODEL_NAME,\n",
    "    openai_base_url=\"http://localhost:8000\",\n",
    "    temperature=0.0\n",
    ")\n",
    "service_context = ServiceContext.from_defaults(llm=llm)\n",
    "set_global_service_context(service_context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53eeb9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_df = pd.read_csv(\"eval_qa50.csv\")\n",
    "evaluator = RagasEvaluator(\n",
    "    metrics=[\"faithfulness\", \"answer_relevancy\", \"context_recall\"]\n",
    ")\n",
    "results = evaluator.evaluate_dataset(\n",
    "    dataset=qa_df.to_dict('records'),\n",
    "    retriever=retriever,\n",
    "    llm=llm\n",
    ")\n",
    "print(results.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082e21fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System metrics\n",
    "start = time.perf_counter()\n",
    "_ = llm.complete(\"Hello\", max_tokens=128)\n",
    "elapsed = time.perf_counter() - start\n",
    "tok_per_sec = 128 / elapsed\n",
    "vram_gb = torch.cuda.max_memory_allocated() / 1e9\n",
    "print(f\"{MODEL_NAME} → {tok_per_sec:.1f} tok/s, {vram_gb:.2f} GB VRAM\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f34a7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project=\"logisticgpt\", name=MODEL_NAME, mode=\"offline\")\n",
    "wandb.log({\n",
    "    \"tok_per_sec\": tok_per_sec,\n",
    "    \"vram_gb\": vram_gb,\n",
    "    **results.mean().to_dict()\n",
    "})\n",
    "wandb.finish()\n",
    "print(\"Logged to wandb (offline).\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
